# LLM Group Study Report

## Introduction

This study group aims to enhance understanding of Large Language Models (LLMs) and learn related technologies.  
Starting with the comparison, introduction, and overview of LLM models, the ultimate goal is to conduct a project.

## Target Audience

* Individuals interested in LLMs  
* Individuals with a basic understanding of Python programming  
* Individuals with a basic understanding of machine learning and deep learning  

## Methodology

* Online (Discord room)  
* Weekly regular meetings (Every Wed. 21:00 ~ 22:00 (UTC +09:00))  
* Focus on presentations, discussions, and practical exercises  

## Schedule

| Week | Learning Content | Objectives | Resources |
|---|---|---|---|
| Week 1 | LLM Overview | Understand the basic concepts of LLMs | [Resource 1](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(1%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EA%B0%9C%EB%A1%A0%20%EB%B0%8F%20%EC%82%AC%EC%9A%A9).pdf) |
| Week 2 | LLM Model Comparison (GPT, Claude, etc.) | Compare the characteristics of various LLM models | [Resource 2](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(2%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EB%B9%84%EA%B5%90).pdf) |
| Week 3 | Prompt Injection | Understand prompt injection attacks and learn defense strategies | [Resource 3](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(3%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EA%B3%B5%EA%B2%A9).pdf) |
| Week 4 | Analysis of LLM processes input into tokens | Experimenting with and analyzing how an LLM processes input text into tokens | [Resource 4](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(4%EC%A3%BC%EC%B0%A8%20-%20LLM%20token%20%EC%B2%98%EB%A6%AC%20%EB%B6%84%EC%84%9D).pdf) |
| Week 5 | Embedding words and Cosine Similarity in LLM | Experiencing the relationship between the words via LLM and analyzing mathematically in shallow way | [Resource 5](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(5%EC%A3%BC%EC%B0%A8%20-%20embedding%EA%B3%BC%20cosine%20similarity).pdf) |
| Week 6 | Self-Attention and Similarity Calculation | Understand how attention weights are derived from vector similarities | [Resource 6](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(6%EC%A3%BC%EC%B0%A8%20-%20How%20Attention%20calculates%20similarity).pdf) |
| Week 7 | Scaled Dot Product Attention (SDPA) Mechanics | Analyze and implement the mathematical steps of Scaled Dot Product Attention | [Resource 7](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(7%EC%A3%BC%EC%B0%A8%20-%20How%20Attention%20score%20is%20calculated)%20.pdf) |
## Resource List

* [Resource 1](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(1%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EA%B0%9C%EB%A1%A0%20%EB%B0%8F%20%EC%82%AC%EC%9A%A9).pdf): LLM Overview  
* [Resource 2](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(2%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EB%B9%84%EA%B5%90).pdf): LLM Model Comparison (GPT, Claude, etc.)  
* [Resource 3](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(3%EC%A3%BC%EC%B0%A8%20-%20LLM%20%EA%B3%B5%EA%B2%A9).pdf): Prompt Injection
* [Resource 4](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(4%EC%A3%BC%EC%B0%A8%20-%20LLM%20token%20%EC%B2%98%EB%A6%AC%20%EB%B6%84%EC%84%9D).pdf): Tokenization in LLMs  
* [Resource 5](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(5%EC%A3%BC%EC%B0%A8%20-%20embedding%EA%B3%BC%20cosine%20similarity).pdf): Embedding and Cosine Similarity between words
* [Resource 6](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(6%EC%A3%BC%EC%B0%A8%20-%20How%20Attention%20calculates%20similarity).pdf): How attention calculates similarty
* [Resource 7](https://github.com/aqua1107/LLM-Group-Study-Report/blob/main/study_materials/%EB%B3%B4%EA%B3%A0%EC%84%9C_%EC%9C%A0%EC%A7%80%EC%83%81%20(7%EC%A3%BC%EC%B0%A8%20-%20How%20Attention%20score%20is%20calculated)%20.pdf): The principle of how scaled dot product attention is calculated
## Progress Report

* Week 1: LLM Overview learning completed (Topic: LLM basic concept)  
* Week 2: Comparison of LLM models such as GPT, Claude, etc. completed (Topic: GPT, Claude, or other LLM models comparison)  
* Week 3: Prompt Injection learning completed (Topic: Prompt injection attacks and defense strategies)  
* Week 4: Discussing about exploring and analyzing how an LLM breaks down input text into individual tokens (Topic: Tokenization in LLMs)
* Week 5: (Code available) Experimenting how words are related in a specific LLM (Topic: Embedding and Cosine Similarity between Words)
* Week 6: Learning how attention is calculated from vector similarity (Topic: Self-Attention and Similarity Calculation)
* Week 7: Mathematical breakdown and implementation of Scaled Dot Product Attention (Topic: Scaled Dot Product Attention Mechanics)
